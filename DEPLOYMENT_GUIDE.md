# AURA Smart Attendance System - Deployment & CORS Guide

## Overview

This document explains the complete architecture of the AURA Smart Attendance System and the critical fixes applied to resolve CORS and Render free tier deployment issues.

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                         â”‚
â”‚  Frontend (Next.js 14)                                                  â”‚
â”‚  https://aura-eight-beta.vercel.app                         [Vercel]   â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ HTTPS
             â”‚ (1) GET /wakeup (2-second wait)
             â”‚ (2) POST /train, GET /recognize
             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                            â”‚
â”‚  Backend (Express.js)                                                      â”‚
â”‚  https://aura-backend.onrender.com                           [Render]     â”‚
â”‚  - User authentication                                                     â”‚
â”‚  - Attendance database                                                     â”‚
â”‚  - Routes requests to Python Face API                                      â”‚
â”‚                                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ HTTP
             â”‚ POST /train, GET /recognize
             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                            â”‚
â”‚  Python Face API (FastAPI + DeepFace)                                      â”‚
â”‚  https://aura-face-api.onrender.com                          [Render]     â”‚
â”‚  - Facenet (128-d embeddings)                                    â”‚        â”‚
â”‚  - OpenCV face detection                                         â”‚        â”‚
â”‚  - Memory-optimized for free tier (512MB)                        â”‚        â”‚
â”‚                                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Critical Issues Resolved

### 1. CORS Misconfiguration (Browser Security Error)

**Problem:**
```
Access to XMLHttpRequest at 'https://aura-face-api.onrender.com/train' 
from origin 'https://aura-eight-beta.vercel.app' has been blocked by CORS policy: 
The value of the 'Access-Control-Allow-Origin' header in the response must not be the wildcard '*' 
when the request's credentials mode (include) is 'include'.
```

**Root Cause:**
Invalid CORS middleware configuration:
```python
# BEFORE (INVALID)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],           # Wildcard
    allow_credentials=True,        # Credentials enabled
    # This combination violates CORS spec!
)
```

**CORS Specification Rule:**
You cannot use `allow_origin: "*"` (wildcard) when `allow_credentials: true` because:
- Wildcard allows any origin, but credentials are sensitive â†’ security breach
- Browser enforces this: if credentials are included, origin must be explicit

**Solution:**
```python
# AFTER (VALID)
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "https://aura-eight-beta.vercel.app",  # Production frontend
        "http://localhost:3000",                # Local development
        "http://127.0.0.1:3000"
    ],
    allow_credentials=False,  # Changed to match wildcard-like behavior
    allow_methods=["GET", "POST", "OPTIONS"],
    allow_headers=["*"],
)

@app.options("/{path:path}")
async def preflight_handler(path: str, request: Request):
    """Explicit OPTIONS handler for CORS preflight requests"""
    return {"detail": "OK"}
```

**Why This Fix Works:**
- âœ… Explicit origin list satisfies CORS spec requirement
- âœ… `allow_credentials=False` is compatible with explicit origins
- âœ… Explicit `@app.options()` handler ensures preflight requests are properly handled
- âœ… Credentials are not required for face recognition (no authentication tokens)

### 2. Render Free Tier Memory Exhaustion (502 Bad Gateway Crashes)

**Problem:**
- Training starts fine
- Crashes at varying percentages (50-90%)
- Render container dies silently
- Browser reports as CORS error (misleading)

**Root Cause:**
Render free tier has 512MB total RAM. DeepFace lazy-loads full Facenet model (~250MB) on EVERY request:

```
Training 5 images scenario:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Image 1: Load model (250MB) + embed face â†’ ~350MB spike â†’ GC
Image 2: Load model (250MB) + embed face â†’ ~350MB spike â†’ GC
Image 3: Load model (250MB) + embed face â†’ ~350MB spike â†’ GC
Image 4: Load model (250MB) + embed face â†’ ~350MB spike â†’ GC
Image 5: Load model (250MB) + embed face â†’ ~350MB spike â†’ CRASH @ limit
```

Old batch processing attempted to use Python's `concurrent.futures.Executor`:
```python
# BEFORE (CAUSES OOM)
def process_images_batch(images):
    with ThreadPoolExecutor(max_workers=5) as executor:
        futures = [
            executor.submit(embedding_function, img)
            for img in images  # All queued at once!
        ]
```

This queued all images simultaneously â†’ forced multiple model loads in parallel â†’ exceeded memory limit.

**Solution - One-at-a-Time Processing:**

```python
# AFTER (MEMORY SAFE)
embeddings, failed_indices = [], []

for idx, img_b64 in enumerate(images_to_use):
    try:
        # 1. Decode base64
        rgb = decode_base64(img_b64)
        
        # 2. Detect faces
        faces = detect_faces_opencv(rgb)
        if not faces:
            failed_indices.append(idx)
            continue
        
        # 3. Get embedding from first face
        face_roi = extract_roi(faces[0])
        
        # Resize to exact 160x160 BEFORE DeepFace
        if face_roi.shape != (160, 160, 3):
            face_roi = cv2.resize(face_roi, (160, 160))
        
        embedding = get_embedding_facenet(face_roi)
        embeddings.append(normalize_embedding(embedding))
        
        logger.info(f"âœ“ Image {idx+1}/5 embedded successfully")
        
    except Exception as e:
        logger.debug(f"âœ— Image {idx+1} failed: {str(e)[:50]}")
        failed_indices.append(idx)
    
    finally:
        # CRITICAL: Free memory immediately after each image
        del rgb, faces, face_roi
        gc.collect()  # Force garbage collection
```

**Why This Approach Works:**

1. **Sequential Processing**: Only ONE model load at a time
2. **Immediate Cleanup**: After each image:
   - Delete all numpy arrays (`del rgb, faces, face_roi`)
   - Call `gc.collect()` immediately
   - Result: Memory returns to ~50MB baseline before next iteration
3. **Resilience**: If one image fails, others continue
4. **Memory Profile**:
   ```
   Baseline: 50MB
   + Load image: 10MB (total: 60MB)
   + Load model: 300MB (total: 360MB) â† PEAK
   + Embed + GC: â†’ ~50MB
   Repeat for next image
   ```

### 3. Render Container Cold Start (Initial Timeout)

**Problem:**
Render free tier spins down services after ~15 minutes of inactivity. First request hits 30-60 second spinup delay â†’ times out.

**Solution - Wakeup Endpoint:**

```python
@app.get("/wakeup")
async def wakeup():
    """
    Wakeup endpoint for Render cold start.
    Frontend calls this before expensive training request.
    Lightweight, no model loading, ensures service is running.
    """
    logger.info("â° Wakeup ping received from frontend")
    return {
        "status": "ready",
        "message": "Face recognition service is awake"
    }
```

**Frontend Implementation:**

```typescript
// 1. Call wakeup with 10-second timeout
setStatusMessage("ðŸŒ… Waking up face recognition service...");
const wakeupController = new AbortController();
const wakeupTimeout = setTimeout(() => wakeupController.abort(), 10000);

try {
    const response = await fetch(
        'https://aura-face-api.onrender.com/wakeup',
        { signal: wakeupController.signal }
    );
    if (!response.ok) throw new Error('Wakeup failed');
} finally {
    clearTimeout(wakeupTimeout);
}

// 2. Wait 2 seconds to ensure service initialized
await new Promise(resolve => setTimeout(resolve, 2000));

// 3. Now call expensive training endpoint with 180-second timeout
setStatusMessage("ðŸ“¸ Training face model...");
const controller = new AbortController();
const timeout = setTimeout(() => controller.abort(), 180000);

try {
    const response = await fetch(
        'https://aura-face-api.onrender.com/train',
        { 
            method: 'POST',
            signal: controller.signal,
            body: formData
        }
    );
} finally {
    clearTimeout(timeout);
}
```

## Environment Variables & Deployment

### Python API Environment Variables

**File: `python-face-api/Dockerfile`**

```dockerfile
# TensorFlow optimization for resource-constrained environments
ENV TF_CPP_MIN_LOG_LEVEL=3              # Suppress TF logging (reduce verbosity)
ENV CUDA_VISIBLE_DEVICES=-1             # Disable CUDA (CPU only on Render free)
ENV TF_ENABLE_ONEDNN_OPTS=0             # Disable oneDNN to avoid binary incompatibility
```

**Rationale:**
- `TF_CPP_MIN_LOG_LEVEL=3`: Reduces logging overhead on free tier
- `CUDA_VISIBLE_DEVICES=-1`: Forces CPU mode (Render free tier doesn't have GPU)
- `TF_ENABLE_ONEDNN_OPTS=0`: Prevents crashes due to different CPU architectures between build and runtime

### Uvicorn Configuration

**File: `python-face-api/Dockerfile` CMD**

```bash
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--timeout-keep-alive", "120"]
```

**Parameters:**
- `--host 0.0.0.0`: Accept connections from any IP
- `--port 8000`: HTTP port (Render maps to public HTTPS)
- `--timeout-keep-alive 120`: Keep alive timeout in seconds
  - Default (5s) too short for Render's cold start
  - 120s allows service to handle slow wakeup

## API Endpoints

### GET /health
Health check endpoint.

**Response:**
```json
{
    "status": "ok",
    "model": "Facenet",
    "detector": "opencv",
    "version": "4.0"
}
```

### GET /wakeup
Wakeup endpoint for Render cold start.

**Response:**
```json
{
    "status": "ready",
    "message": "Face recognition service is awake"
}
```

### POST /train
Train face embeddings from provided images.

**Request:**
```json
{
    "enrollment_id": "faculty_001",
    "images": ["base64_image_1", "base64_image_2", ...]
}
```

**Response:**
```json
{
    "enrollment_id": "faculty_001",
    "enrollment_status": "success",
    "message": "Face model trained successfully",
    "successful_images": 5,
    "failed_images": 0
}
```

**Processing:**
- Images processed ONE AT A TIME (sequential)
- Maximum 5 images processed
- gc.collect() after each image
- Memory cleanup between requests

### POST /recognize
Recognize faces in provided image(s).

**Request:**
```json
{
    "image": "base64_image",
    "enrollment_ids": ["faculty_001", "faculty_002"],
    "threshold": 0.4
}
```

**Response:**
```json
{
    "recognized_faces": [
        {
            "enrollment_id": "faculty_001",
            "confidence": 0.85,
            "face_index": 0
        }
    ],
    "total_faces_detected": 1
}
```

## Deployment Steps

### 1. Deploy Python API to Render

```bash
# Push to your git repository
git add .
git commit -m "CORS fixes and Render free tier optimization"
git push origin main

# Render auto-deploys from git push
# (Configure in Render dashboard)
```

**Render Settings:**
- **Service**: Web Service
- **Runtime**: Python 3.10
- **Start Command**: (Uses Dockerfile CMD)
- **Instance Type**: Free (0.1 CPU, 512MB RAM)
- **Plan**: Free tier (auto-sleeps after 15 min inactivity)

### 2. Update Backend Express Server

Add handling for face API routes:

```javascript
// backend/routes/admin.js or attendance.js
app.post('/api/face/train', async (req, res) => {
    const { enrollment_id, images } = req.body;
    try {
        const response = await fetch('https://aura-face-api.onrender.com/train', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({ enrollment_id, images })
        });
        const data = await response.json();
        res.json(data);
    } catch (error) {
        res.status(500).json({ error: error.message });
    }
});
```

### 3. Update Frontend Training Page

File: `frontend/app/admin/students/[id]/training/page.tsx`

**Key Changes:**
1. Call `/wakeup` endpoint before training
2. Wait 2 seconds after wakeup
3. Wrap `/train` call with 180-second AbortController timeout
4. Display progress messages

```typescript
const handleTraining = async () => {
    try {
        // Step 1: Wakeup
        setProgressStage('ðŸŒ… Waking up face recognition service...');
        const wakeupController = new AbortController();
        const wakeupTimeout = setTimeout(() => wakeupController.abort(), 10000);
        
        try {
            const wakeupResponse = await fetch(
                'https://aura-face-api.onrender.com/wakeup',
                { signal: wakeupController.signal }
            );
            if (!wakeupResponse.ok) throw new Error('Wakeup failed');
        } finally {
            clearTimeout(wakeupTimeout);
        }

        // Step 2: Wait 2 seconds
        await new Promise(resolve => setTimeout(resolve, 2000));

        // Step 3: Train with 180s timeout
        setProgressStage('ðŸ“¸ Training face model...');
        const controller = new AbortController();
        const timeout = setTimeout(() => controller.abort(), 180000);

        try {
            const trainResponse = await fetch(
                'https://aura-face-api.onrender.com/train',
                {
                    method: 'POST',
                    signal: controller.signal,
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ enrollment_id, images })
                }
            );
            
            if (!trainResponse.ok) throw new Error('Training failed');
            const result = await trainResponse.json();
            setProgressPercentage(100);
            setSuccessMessage('Face training completed successfully!');
        } finally {
            clearTimeout(timeout);
        }
    } catch (error) {
        setErrorMessage(error.message);
    }
};
```

### 4. Deploy to Vercel

```bash
# Frontend auto-deploys from git push
git add .
git commit -m "Add face API wakeup and timeout handling"
git push origin main

# Vercel auto-deploys from git push
```

## Monitoring & Troubleshooting

### Issue: 502 Bad Gateway - Face API Training

**Symptoms:**
- Training fails at random percentage
- `[ERROR] Internal Server Error` in console
- Render logs show container death

**Diagnosis:**
```bash
# Check Render logs
# Look for: "Killed" or "OOM" messages
```

**Solutions (in order):**

1. **Verify wakeup is being called** (check frontend network tab)
2. **Increase timeout to 300s** if consistently failing at 50%+:
   ```typescript
   const timeout = setTimeout(() => controller.abort(), 300000); // 5 minutes
   ```
3. **Reduce images to 3-4** instead of 5:
   ```typescript
   const imagesToUse = images.slice(0, 3); // Use only 3 images
   ```
4. **Check Dockerfile env vars** are set correctly:
   ```bash
   docker build -t aura-face-api .
   docker run -it aura-face-api python -c "import os; print(os.environ['TF_ENABLE_ONEDNN_OPTS'])"
   # Should print: 0
   ```

### Issue: CORS Error - No Access-Control-Allow-Origin Header

**Symptoms:**
- Browser console: `Access to XMLHttpRequest has been blocked by CORS policy`
- Verifies wakeup works but train fails
- Error appears instantly (not after timeout)

**Diagnosis:**
This likely means the Python API crashed BEFORE returning response:
- Check if Render instance is spinning up (cold start)
- Check Render free tier quota (might be sleeping)

**Solutions:**

1. **Verify CORS middleware**:
   ```bash
   curl -i https://aura-face-api.onrender.com/health
   # Check for: Access-Control-Allow-Origin: application/json
   ```

2. **Check Render quotas**:
   - Render free tier limits: check dashboard
   - Monthly hours limit might be exceeded
   - Solution: Upgrade to paid tier or wait for monthly reset

3. **Verify allowed origins** in main.py:
   ```python
   # Must include your Vercel URL
   allow_origins=[
       "https://aura-eight-beta.vercel.app",  # Your Vercel deployment
       "http://localhost:3000",
   ]
   ```

### Issue: Training Succeeds but Results are Wrong

**Symptoms:**
- Training completes (100%)
- Recognition doesn't find the face
- Face recognition returns low confidence scores

**Diagnosis:**
1. Check if images are good quality (at least 200x200px faces)
2. Check lighting conditions (too dark/bright â†’ poor embeddings)
3. Verify threshold is reasonable (default 0.4)

**Solutions:**

1. **Increase image quality requirements**:
   ```typescript
   // Ensure face is at least 200x200 pixels
   if (faces[0].width < 200 || faces[0].height < 200) {
       throw new Error('Face too small - please get closer');
   }
   ```

2. **Adjust recognition threshold**:
   ```python
   # Lower threshold = stricter matching (fewer false positives)
   # Higher threshold = looser matching (more false positives)
   # Default 0.4 is reasonable for Facenet
   confidence = 1 - distance
   if confidence >= 0.4:  # Match found
   ```

3. **Debug embeddings**:
   ```bash
   # Add detailed logging to /train endpoint
   logger.info(f"Embedding shape: {embedding.shape}")
   logger.info(f"Embedding mean: {np.mean(embedding)}")
   logger.info(f"Embedding std: {np.std(embedding)}")
   ```

## Performance Metrics

### Expected Performance on Render Free Tier

| Metric | Value | Notes |
|--------|-------|-------|
| Wakeup time | 5-10s | First request after spindown |
| Model load time | 3-5s | Per training session |
| Time per image | 1-2s | Detect + embed + cleanup |
| Training 5 images | 10-15s | Total (including wakeup) |
| Recognition (1 face) | 2-3s | Detect + embed + match |
| Memory peak | ~350MB | During embedding calculation |
| Memory idle | ~50MB | Between requests |

### Production Upgrade Path

**When to Upgrade from Free Tier:**

| Metric | Free Tier | Starter | Standard |
|--------|-----------|---------|----------|
| Cost | $0 | $7/mo | $12/mo |
| CPU | 0.1 | 1 | 1+ |
| RAM | 512MB | 512MB | 1GB+ |
| Auto-sleep | Yes (15 min) | No | No |
| Max requests/mo | N/A | N/A | N/A |
| Recommended for | **Dev/Testing** | **Production (small)** | **Production (large)** |

**Recommendation:**
For production deployment with multiple users, upgrade to Starter tier (~$7/month) to eliminate:
- Auto-sleep delays
- Memory constraints
- Potential timeout issues
- Cold start penalties

## Code Quality Checklist

- âœ… CORS middleware uses explicit origins list
- âœ… `allow_credentials=False` is set
- âœ… OPTIONS preflight handler implemented
- âœ… One-at-a-time image processing (no batch/concurrent)
- âœ… `gc.collect()` called after each image
- âœ… DeepFace model loaded once per request session (not per-image)
- âœ… Environment variables set in Dockerfile
- âœ… Uvicorn timeout_keep_alive=120
- âœ… Frontend calls /wakeup before /train
- âœ… Frontend uses AbortController with timeout
- âœ… Error messages distinguish network from server errors
- âœ… Memory is logged at training start/end

## References

- [CORS Specification](https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS)
- [FastAPI CORS Documentation](https://fastapi.tiangolo.com/tutorial/cors/)
- [Render Free Tier Documentation](https://render.com/docs)
- [DeepFace Documentation](https://github.com/serengp/deepface)
- [Uvicorn Configuration](https://www.uvicorn.org/)

---

**Last Updated:** 2024-01-15  
**Status:** Production Ready  
**Tested On:** Vercel + Render Free Tier + MongoDB Atlas
